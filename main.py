from numpy.random import randn
import numpy as np
import matplotlib.pyplot as plt


def select_action(q_value, epsilon):
    # Generate a random number between 0 and 1
    rand_num = np.random.random()

    # Check if the random number is within 1 - epsilon to 1
    if rand_num <= (1 - epsilon):
        best_actions = np.where(q_value == np.amax(q_value))[0]
        return np.random.choice(best_actions)
    else:
        return np.random.randint(len(q_value))


def get_last(l):
    if not l:
        return 0
    else:
        return l[-1]


def run_bandit(q_star, epsilons, steps):
    action_number = len(q_star)
    epsilon_count = len(epsilons)

    q_value = [np.zeros(action_number) for _ in range(epsilon_count)]
    n = [np.zeros(action_number) for _ in range(epsilon_count)]

    average_rewards = [[] for _ in range(epsilon_count)]

    count_best = [0 for _ in range(epsilon_count)]
    percentage_best = [[] for _ in range(epsilon_count)]

    for i in range(steps):
        for eps_idx, epsilon in enumerate(epsilons):

            selected_action = select_action(q_value[eps_idx], epsilon)

            # random reward is generated with the true value as the mean and variance 1
            reward = np.random.normal(q_star[selected_action], 1)
            # creates a graph with the average reward generated by the program
            average_reward = (get_last(average_rewards[eps_idx]) * sum(n[eps_idx]) + reward) / (sum(n[eps_idx]) + 1)
            average_rewards[eps_idx].append(average_reward)

            # increases action count in selected iteration by 1
            n[eps_idx][selected_action] = n[eps_idx][selected_action] + 1
            # update
            q_value[eps_idx][selected_action] = q_value[eps_idx][selected_action] + (1 / n[eps_idx][selected_action]) * (
                    reward - q_value[eps_idx][selected_action])

            # % of times the best choice was chosen
            if selected_action == np.argmax(q_star):
                count_best[eps_idx] += 1
            percentage_best[eps_idx].append(count_best[eps_idx] / np.sum(n[eps_idx]))

        # random walks, non-stationary
        random_walk = np.random.normal(0, 0.01, 10)
        q_star += random_walk

    return average_rewards, percentage_best


def average_bandit(epsilons, steps, times):
    x_coords = np.arange(steps)
    average_rewards = [[0 for _ in range(steps)] for _ in range(len(epsilons))]
    percentage_best = [[0 for _ in range(steps)] for _ in range(len(epsilons))]

    for i in range(times):
        q_star = randn(10)
        average_rewards_temp, percentage_best_temp = run_bandit(q_star, epsilons, steps)

        for eps_idx in range(len(epsilons)):
            average_rewards[eps_idx] = [sum(value) for value in zip(average_rewards[eps_idx], average_rewards_temp[eps_idx])]
            percentage_best[eps_idx] = [sum(value) for value in zip(percentage_best[eps_idx], percentage_best_temp[eps_idx])]

    for eps_idx in range(len(epsilons)):
        average_rewards[eps_idx] = [value/times for value in average_rewards[eps_idx]]
        percentage_best[eps_idx] = [value/times for value in percentage_best[eps_idx]]

    fig, (r1, p1) = plt.subplots(1, 2, figsize=(12, 6))
    for eps_idx, epsilon in enumerate(epsilons):
        r1.plot(x_coords, average_rewards[eps_idx], label=f"epsilon={epsilon}")
        p1.plot(x_coords, percentage_best[eps_idx], label=f"epsilon={epsilon}")

    r1.set_xlabel("Steps")
    r1.set_ylabel("Average Reward")
    r1.set_title("Multi-epsilon Greedy: Average Reward")
    r1.legend()
    p1.set_xlabel("Steps")
    p1.set_ylabel("% Optimal Action")
    p1.set_title("Multi-epsilon Greedy: % Optimal Action")
    p1.legend()
    plt.show()


epsilons = [0, 0.01, 0.1]
steps = 10000
times = 200
average_bandit(epsilons, steps, times)
